# Sequential Models Configuration (P4 - LSTM/Transformer)

lstm:
  hidden_size: 128
  num_layers: 2
  dropout: 0.3
  bidirectional: false
  learning_rate: 0.001
  batch_size: 32
  epochs: 50
  sequence_length: 10

transformer:
  d_model: 128
  nhead: 8
  num_encoder_layers: 4
  num_decoder_layers: 4
  dim_feedforward: 512
  dropout: 0.1
  learning_rate: 0.0001
  batch_size: 32
  epochs: 50
  max_sequence_length: 512

# Time Series specific
timeseries:
  window_size: 30
  forecast_horizon: 14
  stride: 1
  features: ["value"]
  target: "value"

# Text specific
text:
  max_vocab_size: 10000
  max_length: 512
  tokenizer: "basic"  # basic, wordpiece, bpe
  padding: "max_length"
  truncation: true

# Training
training:
  val_split: 0.2
  early_stopping_patience: 10
  gradient_clip: 1.0
