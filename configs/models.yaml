# Model Configuration

classic:
  random_forest:
    n_estimators: 100
    max_depth: 10
    min_samples_split: 5
    random_state: 42

  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    objective: "reg:squarederror"
    random_state: 42

neural:
  mlp:
    hidden_layers: [128, 64, 32]
    activation: "relu"
    dropout_rate: 0.3
    learning_rate: 0.001
    batch_size: 32
    epochs: 50

  lstm:
    hidden_size: 128
    num_layers: 2
    dropout: 0.3
    learning_rate: 0.001
    batch_size: 32
    epochs: 50
    sequence_length: 10

  transformer:
    d_model: 128
    nhead: 8
    num_layers: 4
    dim_feedforward: 512
    dropout: 0.1
    learning_rate: 0.0001
    batch_size: 32
    epochs: 50

  gnn:
    hidden_channels: 64
    num_layers: 3
    dropout: 0.5
    learning_rate: 0.01
    epochs: 100

  vae:
    latent_dim: 20
    hidden_dims: [128, 64]
    learning_rate: 0.001
    batch_size: 128
    epochs: 100
    beta: 1.0  # KL divergence weight

rl:
  q_learning:
    learning_rate: 0.1
    discount_factor: 0.95
    epsilon: 0.1  # exploration rate
    num_episodes: 1000
    max_steps_per_episode: 200
